{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJMhC57gWPoh"
      },
      "source": [
        "# Cours 11 : Scraping et APIs\n",
        "\n",
        "Le web scraping est une technique d'extraction automatique de données depuis des sites web. En sciences humaines, on peut l'utiliser pour constituer des jeux de données (fichiers excel...) ou des corpus.\n",
        "\n",
        "Dans ce cours, nous allons utiliser Python et quelque librairies qui nous permettront d'explorer des pages web, récupérer leur contenu et extraire les éléments qui nous interessent pour ensuite les sauvegarder dans des formats structurés exploitables.\n",
        "\n",
        "Pour plus d'information au sujet du web scraping, vous pouvez consulter [cet article](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
        "\n",
        "## Structure d'une page web\n",
        "\n",
        "Avant d'entamer le processus de scraping, il est important de comprendre la structure du site web.\n",
        "\n",
        "Les pages web sont un ensemble de HTML (pour la structure), CSS (pour l'habillage) et finalement le JavaScript (pour l'interaction).\n",
        "\n",
        "Voici un exemple de code HTML :\n",
        "\n",
        "```\n",
        "<html> <!-- Ceci est un commentaire , comme pour python mais sans le # -->\n",
        "    \n",
        "    <head> <!-- la balise style permet d'y ecrire du css -->\n",
        "        <style>\n",
        "            .ma-classe{\n",
        "                color:red;\n",
        "            }\n",
        "        </style>\n",
        "\n",
        "        <title>Ma-super-page.com</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1 class=\"ma-classe\">Mon titre principale</h1>\n",
        "        <p>Mon paragraphe</p>\n",
        "        <h2>Mon sous title</h2>\n",
        "        ...\n",
        "\n",
        "        <script>\n",
        "// ici nous pourons ecrire du code javascript pour ajouter de l'animation ou de l'interaction\n",
        "        </script>\n",
        "    </body>\n",
        "</html>\n",
        "<!-- -->\n",
        "```\n",
        "\n",
        "Rassurez-vous, nous resterons sur du Python, mais il est important de voir comment se structure une page web afin de pouvoir la manipuler. Pour plus d'information à ce sujet, vous pouvez consulter [cette page](https://developer.mozilla.org/fr/docs/Learn_web_development/Core/Structuring_content).\n",
        "\n",
        "Pour commencer, nous allons utiliser un outil présent sur toutes nos machines : notre navigateur et son inspecteur !\n",
        "\n",
        "Prenons cette page : https://fr.wikipedia.org/wiki/UCLouvain et essayons d'en explorer la structure.\n",
        "\n",
        "une fois sur la page utilisez le clic droit de votre souris et selectionnez l'inspecteur (_Inspect_ ou _Inspecter la page_).\n",
        "\n",
        "À gauche, nous pouvons observer la structure HTML de la page, composée de différents blocs que nous désignons sous les termes de balises ou tags. À droite se trouvent les éléments CSS associés aux divers blocs HTML.\n",
        "\n",
        "Pour faciliter l'exploration de chacun de ces blocs, notre inspecteur met à notre disposition plusieurs outils utiles. \n",
        "\n",
        "L'inspecteur permet aussi de modifier des éléments de la page (un titre, la couleur d'un paragraphe, une couleur de fond, une image...). On peut par exemple modifier le contenu d'une des balises en utilisant l'outil de sélection: changeons le titre de la page en \"UCL\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIdDYaKsWPoi"
      },
      "source": [
        "## Les outils de scraping avec Python\n",
        "\n",
        "Maintenant que nous avons exploré une page web manuellement, nous allons voir comment l'explorer avec du code Python afin d'automatiser ce travail. Pour ce faire, nous allons télécharger et installer deux bibliothèques indispensables pour le scraping de pages web : `requests` et `beautifulsoup`.\n",
        "\n",
        "- `requests` nous permet d'effectuer des requêtes HTTP, et donc d'accéder aux pages web.- `beautifulsoup` nous aide à récupérer le contenu des différentes balises ou l'intégralité de la page dans des variables Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRtv7FeNWPoj"
      },
      "outputs": [],
      "source": [
        "!pip install bs4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfBsg_a-WPoj"
      },
      "source": [
        "## Scrapons !\n",
        "\n",
        "Dans les sections suivantes, vous trouverez les différentes étapes pour accéder à une page et récupérer le contenu de certaines balises. L'objectif ici est d'extraire le contenu que nous avons préalablement identifié en inspectant la page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W77sA_x1WPoj",
        "outputId": "ba7d820d-0d7b-42cb-ad64-52944f0d9d3c"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url ='https://fr.wikipedia.org/wiki/UCLouvain'\n",
        "page = requests.get(url) # nous essayons juste d'acceder a la page web , comme si il s'agissait de notre navigateur\n",
        "\n",
        "soup = BeautifulSoup(page.text,'html') # On charge le contenu de la page dans BeautifulSoup\n",
        "print(soup.prettify()) # nous affichons l'entiereté de la page html (on utilise une fonction prettify pour rendre le tout un peu plus lisible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons ensuite récupérer 2 informations: le titre de la page, ainsi que son contenu texte. Pour ce faire, nous allons utiliser les balises `h1` (titre) et `p` (paragraphe). Nous allons itérer à travers les paragraphes, et pour chacun de ceux-ci, imprimer son contenu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gzKxL9nWj30"
      },
      "outputs": [],
      "source": [
        "title = soup.find('h1') # je lui demande de retrouver la premiere balise h1\n",
        "print(title.text) # je demande d'afficher le text contenu dans la balise si je ne le precise pas que va t'il ecrire selon vous ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for p in soup.find_all('p'):\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercice: récupération des articles de la page d'accueil de la RTBF\n",
        "\n",
        "Pour cet exercice, nous allons créer un robot qui va ouvrir la page d'accueil du site de la[RTBF](https://www.rtbf.be/), récupérer le titre de tous les articles du jour et les stocker dans un fichier csv.\n",
        "\n",
        "Commençons par importer les librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Récupération de tous les articles de la page d'accueil\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "articles = []\n",
        "\n",
        "root_url = f\"https://www.rtbf.be\"\n",
        "response = requests.get(root_url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "for link in soup.find_all(\"a\", {\"class\":\"stretched-link outline-none\"}):\n",
        "    title = link.text.strip()\n",
        "    url = root_url + link.get('href')\n",
        "    articles.append([url, title])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichage du nombre d'articles récupérés\n",
        "len(articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichage des 10 premières entrées\n",
        "articles[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Création d'un dataframe avec les liens et les titres des articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(articles, columns=['link', 'title'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarde du dataframe dans un fichier csv\n",
        "df.to_csv(f\"rtbf_{time.strftime('%Y%m%d')}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Téléchargement d'un article et affichage du texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Récupération du premier lien du dataframe\n",
        "article_url = df.iloc[0][0]\n",
        "\n",
        "# Ouverture de la page\n",
        "response = requests.get(article_url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "# Parsing du contenu\n",
        "title = soup.find(\"h1\").get_text(separator=' ')\n",
        "\n",
        "paragraphs = []\n",
        "for paragraph in soup.find_all(\"p\", attrs={\"class\": None}):\n",
        "    paragraphs.append(paragraph.get_text(separator=' ').strip())\n",
        "\n",
        "content = \"\\n\".join(paragraphs)\n",
        "print(title)\n",
        "print(\"==================================\")\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nettoyage du texte à l'aide d'expressions régulières"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppression de tout ce qui se trouve entre parenthèses\n",
        "clean_content = re.sub(\"\\([^\\)]+\\)\", \" \", content)\n",
        "# Suppression des espaces multiples\n",
        "clean_content = re.sub(\"\\s+\", \" \", clean_content)\n",
        "\n",
        "print(content)\n",
        "print(\"==================================\")\n",
        "print(clean_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Création d'un fichier avec le contenu de l'article\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"rtbf_example.txt\", \"w\") as writer:\n",
        "    writer.write(f\"{title}\\n\\n{clean_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilisation d'APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API (Application Programming Interface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une API (Interface de Programmation d’Application) constitue une alternative puissante au web scraping pour la création de jeux de données ou de corpus. Il s’agit d’un ensemble de méthodes qui permettent à des applications de communiquer entre elles, de manière structurée et fiable.\n",
        "\n",
        "Alors qu’un humain interagit avec un logiciel via une interface utilisateur, une API permet à des programmes ou des services d’interagir directement entre eux. Par exemple, plutôt que d’extraire les informations visibles sur une page web (comme on le ferait avec du scraping), on peut interroger directement l’API d’un service pour obtenir des données propres, souvent mieux structurées et plus complètes.\n",
        "\n",
        "De nombreuses entreprises mettent à disposition des API — gratuites ou payantes — qui permettent d’accéder à leurs données de manière automatisée. Il est souvent nécessaire de s’y authentifier, mais cela ouvre la porte à la création efficace de datasets sans avoir à manipuler le code HTML des pages web.\n",
        "\n",
        "\n",
        "### Comment appeler une API ?\n",
        "\n",
        "Chaque API a ses propres spécifications. En résumé, voici ce qu’il faut connaître :\n",
        "\n",
        "- L’**URL racine**, qui définit l’adresse de l’API  \n",
        "- Une **méthode** (les plus courantes sont `GET` et `POST`, mais il en existe beaucoup d'autres [ici](https://restfulapi.net/http-methods/))  \n",
        "- Un **endpoint**, dont le rôle est comparable à une fonction en Python  \n",
        "- Des **paramètres**, que l’on peut assimiler aux paramètres d’une fonction en Python  \n",
        "\n",
        "En pratique, on peut tester une API directement dans le navigateur (au moins pour les méthodes `GET`). Un outil très utile pour tester des appels API plus complexes est [Postman](https://www.postman.com/).\n",
        "\n",
        "Dans notre cas, nous utiliserons bien sûr Python et notre fameuse bibliothèque `requests`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice\n",
        "\n",
        "On va utiliser [newsapi](https://newsapi.org/) pour récupérer les derniers articles du journal français [L'Equipe](https://www.lequipe.fr/).\n",
        "\n",
        "Dans notre cas les spécifications sont les suivantes :\n",
        "\n",
        "- URL racine: `https://newsapi.org/v2`\n",
        "- Méthode: `GET`\n",
        "- Endpoint: `top-headlines`\n",
        "- Paramètres (clé: valeur):\n",
        "    - `apiKey`: `73bbb95f8ecb49b499113a46481b4af1` (this credential key has been created for you)\n",
        "    - `sources`: `lequipe`\n",
        "\n",
        "Dans le navigateur, voici la manière de traduire cette url : `{ROOT_URL}/{ENDPOINT}?{key}={value1}&{key2}={value2}`\n",
        "\n",
        "Essayons ensemble de construire cette URL et de l'ouvrir dans le navigateur.\n",
        "\n",
        "#### Et en Python ?\n",
        "\n",
        "Pour faire ceci on peut directement utiliser la librairie `requests`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "root_url = \"\"\n",
        "endpoint = \"\" \n",
        "params = {\n",
        "    \"apiKey\": \"\",\n",
        "    \"sources\": \"\"\n",
        "}\n",
        "# Let's call the get method of requests on our specifications\n",
        "response = requests.get(f\"{root_url}/{endpoint}\", params=params)\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le résultat est un peu indigeste, non ? Pourquoi ne pas analyser (parser) la sortie JSON pour obtenir uniquement les titres des articles dans une liste ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
