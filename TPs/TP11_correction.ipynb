{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3: Scraping et APIs\n",
    "\n",
    "\n",
    "Ce notebook reprend 2 exercices à faire chez vous. Chaque exercice vous demande de réaliser une série d'opérations. Utilisez les concepts vus au cours pour réaliser ce qui vous est demandé.\n",
    "\n",
    "Nous passerons à travers les exercices lors de la séance de TP sur Teams. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Utilisation d'APIs\n",
    "\n",
    "- Cherchez sur Internet une API qui pourrait vous être utile dans le cadre de votre projet\n",
    "- Faites un appel à cette API à travers un script Python et exportez le résultat dans le format de fichier adéquat (`txt`, `json`, `xlsx`, `csv`...)\n",
    "\n",
    "Quelques sources d'inspiration:\n",
    "- https://free-apis.github.io/#/categories\n",
    "- https://www.odwb.be/pages/home/\n",
    "- https://huggingface.co/docs/inference-providers/index \n",
    "\n",
    "Il en existe plein d'autres ! Soyez créatifs :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "API_KEY = \"dfc2e6777d40b9512ec6c937fc328736929dab6ffb0b186aadf86045\"\n",
    "\n",
    "headers = {\n",
    "    \"X-TextRazor-Key\": API_KEY\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"extractors\": \"entities\",\n",
    "    \"text\": \"L'UCLouvain est une université belge francophone située à Louvain-la-Neuve, en Wallonie\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.post(\"https://api.textrazor.com/\", headers=headers, data=data)\n",
    "\n",
    "result = []\n",
    "for entity in response.json()[\"response\"][\"entities\"]:\n",
    "    print(entity[\"entityId\"], entity[\"wikidataId\"])\n",
    "    \n",
    "    parameters = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": entity[\"wikidataId\"],\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    \n",
    "    wikidata_response = requests.get(f\"https://www.wikidata.org/w/api.php\", params=parameters)\n",
    "    name_nl = wikidata_response.json()[\"entities\"][entity[\"wikidataId\"]][\"labels\"][\"nl\"][\"value\"]\n",
    "    result.append({\n",
    "        \"name\": entity[\"entityId\"],\n",
    "        \"wikidata_id\": entity[\"wikidataId\"],\n",
    "        \"name_nl\": name_nl\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(result)\n",
    "df.to_excel(\"entities.xlsx\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, nous vous proposons de créer un répertoire de citations. Concrètement, vous allez extraire des citations depuis le site https://quotes.toscrape.com, un site spécialement conçu pour les exercices de scraping. Vous stockerez ensuite les données dans un fichier csv.\n",
    "\n",
    "Voici les étapes à implémenter:\n",
    "\n",
    "1. Ouvrir les 10 premières page du site avec `requests` (cela nécessitera sans doute une boucle)\n",
    "2. A l'aide de `beautifulsoup`, Extraire chaque citation, avec les informations suivantes:\n",
    "- L'auteur ou l'autrice\n",
    "- Le texte de la citation\n",
    "- La liste des tags associés\n",
    "3. Stocker ces citations dans un DataFrame avec 3 colonnes\n",
    "4. Exporter le DataFrame dans un fichier `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_textrazor_entities(text):\n",
    "\n",
    "    headers = {\n",
    "        \"X-TextRazor-Key\": API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"extractors\": \"entities\",\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.textrazor.com/\", headers=headers, data=data)\n",
    "    wikidata_ids = []\n",
    "    for entity in response.json()[\"response\"].get(\"entities\", []):\n",
    "        wikidata_ids.append(entity.get(\"wikidataId\"))\n",
    "    return wikidata_ids\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "quotes = []\n",
    "for i in range(1, 3):\n",
    "    \n",
    "    print(f\"Page {i}\")\n",
    "    response = requests.get(f\"https://quotes.toscrape.com/page/{i}\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "        text = quote.find(\"span\", class_=\"text\").text \n",
    "        author = quote.find(\"small\", class_=\"author\").text\n",
    "        \n",
    "        tags = [tag.text for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "        \n",
    "        quotes.append({\n",
    "            \"text\": text,\n",
    "            \"author\": author,\n",
    "            \"tags\": \", \".join(tags),\n",
    "            \"wikidata_ids\": get_textrazor_entities(text)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(quotes)\n",
    "df.to_excel(\"quotes.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
